# -*- coding: utf-8 -*-
"""supervised_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MoxME5HCD-83s3TRkfvnC04uCVG2KgSw
"""

# Connect to the drive
from google.colab import drive
drive.mount('/content/drive')

# Import all the required libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import MaxNLocator
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score


from sklearn.model_selection import GridSearchCV

# Loading the csv
df = pd.read_csv("/content/drive/MyDrive/Suoervised_Learning/INNHotelsGroup.csv")

df.head()

df.info()

# Nnne of the columns have empty values
df.isnull().sum()

df.describe()

"""Based on this description we can observe that
Lead time has some outliers
Average Price Per Room also has outliers.
"""

df_new = pd.get_dummies(data = df, columns = ['type_of_meal_plan','market_segment_type','booking_status','room_type_reserved'], prefix='Col',dtype=int)
df_new.head()
df_new.info()

df = pd.concat([df, one_hot_encoded], axis=1)

df.head()

df.drop(columns=['type_of_meal_plan','room_type_reserved','market_segment_type','booking_status'],inplace=True)

df.describe()

df.info()

df_new['arrival_month'].plot(kind='hist')

"""Using this histogram of the column arrival_month, we can conclude that December is the most busiest month.

"""

df_new.info()

sum_columns = df_new[['Col_Aviation','Col_Complementary','Col_Corporate','Col_Offline','Col_Online']].sum()

sum_columns

"""From this we can conclude that the maximum guests come from the market segment which is Online"""

df.info()

sns.barplot(x = 'market_segment_type',y = 'avg_price_per_room',data = df)

"""From this barplot we can observe that, avg room price is least for market segment complimentary, followed by corporate, then offline, then aviation, and the highest being online market segment."""

df['booking_status'].value_counts()

percentage = df['booking_status'].value_counts(normalize=True) * 100
print(percentage)

"""The percentage of bookings that got cancelled are 32.7%"""

rg=df_new.groupby('Col_Canceled')['repeated_guest'].sum()

percentage = (rg/rg.sum()) * 100

print(percentage)

"""1.7% of repeating guests cancelled their bookings."""

sns.barplot(x = 'booking_status',y = 'no_of_special_requests',data = df)
plt.yticks(np.arange(0,5))

"""From this we can observe that if a customer has a special request then they dont cancel their booking, but if they've not made any special requests, they might or might not cancel their booking"""

def missing_check(df):
    total = df.isnull().sum().sort_values(ascending=False)   # total number of null values
    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)  # percentage of values that are null
    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])  # putting the above two together
    return missing_data # return the dataframe
missing_check(df)

#There are no missing values in the dataset.

#Univariate Analysis
def univariate_analysis(df):
    # For Numerical Columns: Histogram and Boxplot
    numerical_cols = df.select_dtypes(include=['number']).columns
    for col in numerical_cols:
        plt.figure(figsize=(12, 5))

        # Histogram
        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
        plt.subplot(1, 2, 1)
        sns.histplot(df[col], kde=True, bins=10)
        plt.title(f'Histogram of {col}')

        # Boxplot
        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
        plt.subplot(1, 2, 2)
        sns.boxplot(x=df[col])
        plt.title(f'Boxplot of {col}')

        plt.show()
univariate_analysis(df)

"""We can observe that in case of Univariate Analysis


1.   Number of Adults : There are mostly 2 Adults
2.   Number of Children : Mostly no children with very few outliers
3.   Weekend Nights : Mostly 0-2 Weekend Nights with few outliers.
4.   No of Week Nights : Range between 0-6 with some outliers.
5.   Car Space : Most people dont require car parking space.
6.   Lead Time : Lead Time ranges mostly between 0-300 days, with 0-50 being  
     the most usual lead time, but there are quite a lot of outliers in this case.
7.   Arrival Month : Most guests coming towards the end of the year during the
     holiday season
8.   Repeated Guests : Very few repeating guests showing most people not
     preferring to return to the hotel
9.   Previous Cancellations : Most customers have none previous cancellations
     with a few outliers
10.  Average Price of Room is around 100
11.  Special Requests :  Maximum Customers have no requests but there are quite a few customers who have 1-3 requests and the customers who have atleast one request never cancel.

"""

df.drop(columns=['Booking_ID'], inplace=True)

#Univariate Anlaysis for Categorical Columns
# 1. Identify all categorical columns (non-numerical columns)
categorical_columns = df.select_dtypes(include=['object']).columns

# 2. Univariate Analysis for Each Categorical Column
for col in categorical_columns:
    print(f"\nUnivariate Analysis for '{col}':")

    # Frequency of unique values
    value_counts = df[col].value_counts()
    print(value_counts)

    # Plot a bar chart
    plt.figure(figsize=(6, 4))
    sns.countplot(x=col, data=df, palette='Set2')
    plt.title(f'Bar Plot of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.xticks(rotation=45)  # Rotate x labels if needed
    plt.show()

    # Optionally: Plot a pie chart
    plt.figure(figsize=(6, 6))
    df[col].value_counts().plot.pie(autopct='%1.1f%%', colors=sns.color_palette('Set2'))
    plt.title(f'Pie Chart of {col}')
    plt.ylabel('')  # Remove ylabel
    plt.show()

"""Univariate Analysis of Categorical Variables

1. Meal Plan : We can observe that most customers have booked Meal Plan 1, followed by Not Selected.
2. Room Type : Most customers have preferred to book Room Type 1, followed by room type 4.
3. Market Segment : Most customers prefer booking through Online Mode, followed by Offline.
4. Booking Status : Most bookings(67.2%) remain in the Non Canceled Category
"""

#Bivariate Analysis
sns.pairplot(df)
plt.show()

# Calculate the correlation matrix

df_reduced = df.drop(columns=['type_of_meal_plan','room_type_reserved','market_segment_type','booking_status'])
correlation_matrix = df_reduced.corr()

# Create a heatmap of the correlation matrix
plt.figure(figsize=(12, 10))  # Optional: Set figure size
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap of Numerical Columns')
plt.show()

#df_new = pd.get_dummies(data = df, columns = ['market_segment_type','booking_status'], prefix='Col',dtype=int)

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html /content/drive/MyDrive/Suoervised_Learning/supervised_learning.ipynb

df_reduced = df_new.drop(columns=['type_of_meal_plan','room_type_reserved'])
correlation_matrix = df_reduced.corr()

# Create a heatmap of the correlation matrix
plt.figure(figsize=(20, 18))  # Optional: Set figure size
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap of Numerical Columns')
plt.show()

"""From the heatmap we can observe that


*   No of children is slighly positively corelated to Avg Price Per Room
*   Lead Time is positively corelated with Booking Status(Cancellations), which is if the lead time is large then the chances of cancellation are also high.


*   Repeated guest is +vely correlated with no of previpus cancellations, no of previous bookings not cancelled and corporate bookings.

*   Price per room is +vely corelated with Online bookings


*   No of special requests is +vely correlated with Online bookings, and non cancellations  
*   Corporate Market Segment is positively corelated with Repeated Guest

#Outlier Treatment
"""

result = df[df['avg_price_per_room'] == 540]
print (result)
#df_dropped = df.drop(index=33115)

df_dropped = df.drop(index=33114)

df_dropped.describe()

"""We have removed the outlier from the dataframe which was causing huge standard deviation in case of average price per room

#Feature Engineering
We have converted the following columns into numerical columns using one hot encoding


1. type_of_meal_plan
2. market_segment_type
3. booking_status
4. room_type_reserved
"""

df_new = pd.get_dummies(data = df_dropped, columns = ['type_of_meal_plan','market_segment_type','room_type_reserved'], prefix='Col',dtype=int)

df_new.info()

"""#Feature Selection before Test-Train Split"""

X = df_new[[
    'no_of_adults',
    'no_of_children',
    'no_of_weekend_nights',
    'no_of_week_nights',
    'lead_time',
    'arrival_month',
    'arrival_date',
    'repeated_guest',
    'no_of_previous_cancellations',
    'no_of_previous_bookings_not_canceled',
    'avg_price_per_room',
    'no_of_special_requests',
    'Col_Meal Plan 1',
    'Col_Meal Plan 2',
    'Col_Meal Plan 3',
    'Col_Not Selected',
    'Col_Aviation',
    'Col_Complementary',
    'Col_Corporate',
    'Col_Offline',
    'Col_Online',
    'Col_Room_Type 1',
    'Col_Room_Type 2',
    'Col_Room_Type 3',
    'Col_Room_Type 4',
    'Col_Room_Type 5',
    'Col_Room_Type 6',
    'Col_Room_Type 7'

]]

y = df_new['booking_status']

# Split the data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets
(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'KNN Classifier': KNeighborsClassifier(),
    'Naive Bayes Classifier': GaussianNB(),
    'SVM Classifier': SVC(probability=True)  # Enable probability for ROC AUC
}

# Evaluate each model
results = {}

LogisticRegression(max_iter=2000)

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]  # For ROC AUC

    results[name] = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'ROC AUC': roc_auc_score(y_test, y_proba),
        'Classification Report': classification_report(y_test, y_pred)
    }

# Display the results
results

performance_df = pd.DataFrame(results).T

performance_df

"""We can observe that


1.  KNN classifier has the highest accuracy in this case
2.  SVM Classifier has the highest precision
3.  Logistic Regression has the highest ROC AUC
4.  Naive Bayes has the highest recall
5.  KNN has the highest f1-score

#Tuning Logit Model
"""

# Define the model
log_reg = LogisticRegression(max_iter=2000)

# Define the parameter grid
param_grid_log_reg = {
    'C': [0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear', 'lbfgs']
}

# Set up Grid Search
grid_search_log_reg = GridSearchCV(log_reg, param_grid_log_reg, scoring='roc_auc', cv=5)
grid_search_log_reg.fit(X_train, y_train)

# Best parameters and score
best_log_reg = grid_search_log_reg.best_estimator_
best_log_reg_score = grid_search_log_reg.best_score_

print(best_log_reg_score)

print(best_log_reg)

"""#Tuning SVM Classifier

"""

# Define the model
svm = SVC(probability=True)

# Define the parameter grid
param_grid_svm = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1],
    'kernel': ['linear', 'rbf', 'poly']
}

# Set up Grid Search
grid_search_svm = GridSearchCV(svm, param_grid_svm, scoring='roc_auc', cv=5)
grid_search_svm.fit(X_train, y_train)

# Best parameters and score
best_svm = grid_search_svm.best_estimator_
best_svm_score = grid_search_svm.best_score_

"""#Tuning KNN Classifier"""

from sklearn.neighbors import KNeighborsClassifier


# Define the model
knn = KNeighborsClassifier()

# Define the parameter grid
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11],
    'weights': ['uniform', 'distance']
}

# Set up Grid Search
grid_search_knn = GridSearchCV(knn, param_grid_knn, scoring='roc_auc', cv=5)
grid_search_knn.fit(X_train, y_train)

# Best parameters and score
best_knn = grid_search_knn.best_estimator_
best_knn_score = grid_search_knn.best_score_

"""#Evaluating Performance Across Metrics"""

# Function to evaluate models
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]  # For ROC AUC
    accuracy = accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)
    classification_report_str = classification_report(y_test, y_pred)
    return accuracy, roc_auc, classification_report_str

# Evaluate each tuned model
log_reg_results = evaluate_model(best_log_reg, X_test_scaled, y_test)
knn_results = evaluate_model(best_knn, X_test_scaled, y_test)
svm_results = evaluate_model(best_svm, X_test_scaled, y_test)

# Compile results
tuning_results = {
    'Logistic Regression': log_reg_results,
    'KNN': knn_results,
    'SVM': svm_results
}

# Display results


performance_df = pd.DataFrame(tuning_results, index=['Accuracy', 'ROC AUC', 'Classification Report']).T
print(performance_df)

"""We can observe that we have tuned all the 3 models by using Grid Search to find the optimal parameters of C, max iterations and solver used.
If we look at the accuracy of Logit Model, we can observe it has increased from almost 80 to 85%.
Similarly if we look at KNN and SVM, we have tuned the paramters and can observe increase in accuracy, precision, recall, F1-score and ROC for all the tuned models.

#Actionable Insights & Recommendations

We can observe that based on the data,
Logistic Regression has the most balanced
performance,based upon precision and recall.


*   Logit is the most simple model compared to SVM and KNN making it the best choice in this case.
*   It is the easiest when it comes to hyper-parameter tuning


*   Although the ROC-AUC are similar, Logit is straightforward hence we can choose this model

#Insights

1. While KNN has good precision, but the low recall might mean it'll fail to capture the not cancelled bookings
2. NVB has good recall but it was less precise
3. SVM and Logit performed almost equally, but logit is more simple


#Actionable Insights

1. Offers: The hotel can offer good deals and discounts to those who might cancel, based on the model, so as to reduce the cancellations.

2. Efficiency : Once the hotel knows which bookings are going to be cancelled, they can manage thier inventory in a much more optimised fashion so as to reduce losses.

3. Predictor : We can improve the model to predict the not cancelled bookings, as more feature engineering can be used to figure out the weather during the time of arrival or flight historical data to predict any cancellations.
"""